{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_d43ruYa2K8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "\n",
        "def createModel_BASE():\n",
        "\n",
        "    # CNN Model Architecture\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(source_params['learning_rate'])\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(x_train_source.shape[1], x_train_source.shape[2])))\n",
        "    model.add(tf.keras.layers.Conv1D(source_params['conv_filters_1'], 5, strides=1, padding='valid', activation='relu')) # 1D convolutional layer with 32 filters, a kernel size of 3, and a ReLU activation function\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Conv1D(source_params['conv_filters_2'], 3, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Conv1D(source_params['conv_filters_3'], 3, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(source_params['dense_units_1'], activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(source_params['dropout_rate_1']))\n",
        "    model.add(tf.keras.layers.Dense(source_params['dense_units_2'], activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(source_params['dropout_rate_2']))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def createModel_BASE_target():\n",
        "\n",
        "    # LSTM Model Architecture\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(target_params['learning_rate'])\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(x_train_target.shape[1], x_train_target.shape[2])))\n",
        "    model.add(tf.keras.layers.Conv1D(target_params['conv_filters_1'], 5, strides=1, padding='valid', activation='relu')) # 1D convolutional layer with 32 filters, a kernel size of 3, and a ReLU activation function\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Conv1D(target_params['conv_filters_2'], 3, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Conv1D(target_params['conv_filters_3'], 3, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(target_params['dense_units_1'], activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(target_params['dropout_rate_1']))\n",
        "    model.add(tf.keras.layers.Dense(target_params['dense_units_2'], activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(target_params['dropout_rate_2']))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def createModel_BASE_target_2():\n",
        "\n",
        "    # LSTM Model Architecture\n",
        "\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(target_2_params['learning_rate'])\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(x_train_target_2.shape[1], x_train_target_2.shape[2])))\n",
        "    model.add(tf.keras.layers.Conv1D(target_2_params['conv_filters_1'], 5, strides=1, padding='valid', activation='relu')) # 1D convolutional layer with 32 filters, a kernel size of 3, and a ReLU activation function\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Conv1D(target_2_params['conv_filters_2'], 3, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Conv1D(target_2_params['conv_filters_3'], 3, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(target_2_params['dense_units_1'], activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(target_2_params['dropout_rate_1']))\n",
        "    model.add(tf.keras.layers.Dense(target_2_params['dense_units_2'], activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(target_2_params['dropout_rate_2']))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def createModel_TL1(domain):\n",
        "\n",
        "    model = tf.keras.models.load_model('Models/CNN/12M/BASE models/EC_BASE_source.keras')\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(source_params['learning_rate'])\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def createModel_TL2(domain):\n",
        "\n",
        "    model = tf.keras.models.load_model('Models/CNN/12M/BASE models/EC_BASE_source.keras')\n",
        "    model.layers[7].trainable = False\n",
        "    model.layers[9].trainable = False\n",
        "\n",
        "    #Compile model\n",
        "    optimizer = tf.keras.optimizers.Adam(source_params['learning_rate'])\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def createModel_TL3(domain):\n",
        "\n",
        "    # LSTM Model Architecture\n",
        "\n",
        "    base_model = tf.keras.models.load_model('Models/CNN/12M/BASE models/EC_BASE_source.keras')\n",
        "\n",
        "    input_shape = base_model.input_shape[1:]\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.Conv1D(source_params['conv_filters_1'], 5, strides=1, padding='valid', activation='relu')(inputs)\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "    x = tf.keras.layers.Conv1D(source_params['conv_filters_2'], 3, activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "    x = tf.keras.layers.Conv1D(source_params['conv_filters_3'], 3, activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    for i in range(7,12):\n",
        "        x = base_model.layers[i](x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "    model.layers[7].trainable = False\n",
        "    model.layers[9].trainable = False\n",
        "\n",
        "    #Compile model\n",
        "    optimizer = tf.keras.optimizers.Adam(source_params['learning_rate'])\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def fit_model_plus_stats_BASE(model, x_train, y_train, domain, type_):\n",
        "\n",
        "    # Early stopping property\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "    if type_ == '6M':\n",
        "        dir_save = 'Models/CNN/6M/BASE models/'\n",
        "    elif type_ == '12M':\n",
        "        dir_save = 'Models/CNN/12M/BASE models/'\n",
        "\n",
        "    if domain == 'source':\n",
        "\n",
        "        # Fit the model\n",
        "        history = model.fit(x_train, y_train, epochs=100, validation_split=0.2, batch_size=source_params['batch_size'], verbose=1, shuffle=True, callbacks=[es]).history\n",
        "\n",
        "        # Save the model\n",
        "        model.save('Models/CNN/12M/BASE models/EC_BASE_source.keras')\n",
        "\n",
        "        #Load the model\n",
        "        model = tf.keras.models.load_model('Models/CNN/12M/BASE models/EC_BASE_source.keras')\n",
        "\n",
        "    elif domain == 'target':\n",
        "\n",
        "        # Fit the model\n",
        "        history = model.fit(x_train, y_train, epochs=100, validation_split=0.2, batch_size=target_params['batch_size'], verbose=1, shuffle=True, callbacks=[es]).history\n",
        "\n",
        "        # Save the model\n",
        "        model.save( dir_save + 'EC_BASE_target.keras')\n",
        "\n",
        "        #Load the model\n",
        "        model = tf.keras.models.load_model(dir_save + 'EC_BASE_target.keras')\n",
        "\n",
        "    elif domain == 'target_2':\n",
        "\n",
        "        # Fit the model\n",
        "        history = model.fit(x_train, y_train, epochs=100, validation_split=0.2, batch_size=target_2_params['batch_size'], verbose=1, shuffle=True, callbacks=[es]).history\n",
        "\n",
        "        # Save the model\n",
        "        model.save( dir_save + 'EC_BASE_target_2.keras')\n",
        "\n",
        "        #Load the model\n",
        "        model = tf.keras.models.load_model(dir_save + 'EC_BASE_target_2.keras')\n",
        "\n",
        "    # summarize history for MAE and MSE\n",
        "    plt.plot(history['loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('Model MSE')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def fit_model_plus_stats_TL1(model, x_train, y_train, domain, type_):\n",
        "\n",
        "    # Early stopping property\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6)\n",
        "\n",
        "    if type_ == '6M':\n",
        "        dir_save = 'Models/CNN/6M/BASE models/'\n",
        "    elif type_ == '12M':\n",
        "        dir_save = 'Models/CNN/12M/BASE models/'\n",
        "\n",
        "    if domain == 'target':\n",
        "\n",
        "        params = target_params\n",
        "\n",
        "        # Fit the model\n",
        "        history = model.fit(x_train, y_train, epochs=50, validation_split=0.2, batch_size=params['batch_size'], verbose=1, shuffle=True, callbacks=[es]).history\n",
        "\n",
        "        # Save the model\n",
        "        model.save( dir_save + 'EC_TL1_target.keras')\n",
        "\n",
        "        #Load the model\n",
        "        model = tf.keras.models.load_model(dir_save + 'EC_TL1_target.keras')\n",
        "\n",
        "    elif domain == 'target_2':\n",
        "\n",
        "        params = target_2_params\n",
        "\n",
        "        # Fit the model\n",
        "        history = model.fit(x_train, y_train, epochs=50, validation_split=0.2, batch_size=params['batch_size'], verbose=1, shuffle=True, callbacks=[es]).history\n",
        "\n",
        "        # Save the model\n",
        "        model.save( dir_save + 'EC_TL1_target_2.keras')\n",
        "\n",
        "        #Load the model\n",
        "        model = tf.keras.models.load_model(dir_save + 'EC_TL1_target_2.keras')\n",
        "\n",
        "    # summarize history for MAE and MSE\n",
        "    plt.plot(history['loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('Model MSE')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def fit_model_plus_stats_TL2(model, x_train, y_train, domain, type_):\n",
        "\n",
        "    # Early stopping property\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6)\n",
        "\n",
        "    if type_ == '6M':\n",
        "        dir_save = 'Models/CNN/6M/BASE models/'\n",
        "    elif type_ == '12M':\n",
        "        dir_save = 'Models/CNN/12M/BASE models/'\n",
        "\n",
        "    if domain == 'target':\n",
        "\n",
        "        params = target_params\n",
        "\n",
        "        # Fit the model\n",
        "        history = model.fit(x_train, y_train, epochs=50, validation_split=0.2, batch_size=params['batch_size'], verbose=1, shuffle=True, callbacks=[es]).history\n",
        "\n",
        "        # Save the model\n",
        "        model.save( dir_save + 'EC_TL2_target.keras')\n",
        "\n",
        "        #Load the model\n",
        "        model = tf.keras.models.load_model(dir_save + 'EC_TL2_target.keras')\n",
        "\n",
        "    elif domain == 'target_2':\n",
        "\n",
        "        params = target_2_params\n",
        "\n",
        "        # Fit the model\n",
        "        history = model.fit(x_train, y_train, epochs=50, validation_split=0.2, batch_size=params['batch_size'], verbose=1, shuffle=True, callbacks=[es]).history\n",
        "\n",
        "        # Save the model\n",
        "        model.save( dir_save + 'EC_TL2_target_2.keras')\n",
        "\n",
        "        #Load the model\n",
        "        model = tf.keras.models.load_model(dir_save + 'EC_TL2_target_2.keras')\n",
        "\n",
        "    # summarize history for MAE and MSE\n",
        "    plt.plot(history['loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('Model MSE')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def fit_model_plus_stats_TL3(model, x_train, y_train, domain, type_):\n",
        "\n",
        "    # Early stopping property\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "    if type_ == '6M':\n",
        "        dir_save = 'Models/CNN/6M/BASE models/'\n",
        "    elif type_ == '12M':\n",
        "        dir_save = 'Models/CNN/12M/BASE models/'\n",
        "\n",
        "    if domain == 'target':\n",
        "\n",
        "        params = target_params\n",
        "\n",
        "        # Fit the model\n",
        "        history = model.fit(x_train, y_train, epochs=50, validation_split=0.2, batch_size=params['batch_size'], verbose=1, shuffle=True, callbacks=[es]).history\n",
        "\n",
        "        # Fine Tune Model\n",
        "        model.trainable = True\n",
        "        model.layers[8].trainable = True\n",
        "        model.layers[10].trainable = True\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(source_params['learning_rate']*0.7) # low to prevent overfitting\n",
        "        model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "\n",
        "        history = model.fit(x_train, y_train, epochs=20, validation_split=0.2, batch_size=params['batch_size'], verbose=1, shuffle=True, callbacks=[es]).history\n",
        "\n",
        "        # Save the model\n",
        "        model.save( dir_save + 'EC_TL3_target.keras')\n",
        "\n",
        "        #Load the model\n",
        "        model = tf.keras.models.load_model(dir_save + 'EC_TL3_target.keras')\n",
        "\n",
        "    elif domain == 'target_2':\n",
        "\n",
        "        params = target_2_params\n",
        "\n",
        "        # Fit the model\n",
        "        history = model.fit(x_train, y_train, epochs=50, validation_split=0.2, batch_size=params['batch_size'], verbose=1, shuffle=True, callbacks=[es]).history\n",
        "\n",
        "        # Fine Tune Model\n",
        "        model.trainable = True\n",
        "        model.layers[7].trainable = True\n",
        "        model.layers[9].trainable = True\n",
        "        optimizer = tf.keras.optimizers.Adam(source_params['learning_rate']*0.7) # low to prevent overfitting\n",
        "        model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "\n",
        "        history = model.fit(x_train, y_train, epochs=20, validation_split=0.2, batch_size=params['batch_size'], verbose=1, shuffle=True, callbacks=[es]).history\n",
        "\n",
        "        # Save the model\n",
        "        model.save( dir_save + 'EC_TL3_target_2.keras')\n",
        "\n",
        "        #Load the model\n",
        "        model = tf.keras.models.load_model(dir_save + 'EC_TL3_target_2.keras')\n",
        "\n",
        "    # summarize history for MAE and MSE\n",
        "    plt.plot(history['loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('Model MSE')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def predict_model(model, x_test):\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    return(y_pred)\n",
        "\n",
        "\n",
        "def percentage_error(actual, predicted):\n",
        "    res = np.empty(actual.shape)\n",
        "    for j in range(actual.shape[0]):\n",
        "        if actual[j] != 0:\n",
        "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
        "        else:\n",
        "            res[j] = predicted[j] / np.mean(actual)\n",
        "    return res\n",
        "\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred))))\n",
        "\n",
        "\n",
        "\n",
        "def model_fit_predict_results(mode, type_):\n",
        "\n",
        "    metrics_unscaled = []\n",
        "\n",
        "    if mode == 'BASE_source':\n",
        "\n",
        "        scaler = scaler_source\n",
        "        y_test = y_test_source[:-24]\n",
        "        sqm = sqm_source\n",
        "\n",
        "        model = createModel_BASE()\n",
        "\n",
        "        fit_model_plus_stats_BASE(model, x_train_source, y_train_source, 'source', type_)\n",
        "\n",
        "        predictions_scaled = predict_model(model, x_test_source)\n",
        "\n",
        "\n",
        "    elif mode == 'BASE_target' or mode == 'TL1_target' or mode == 'TL2_target' or mode == 'TL3_target':\n",
        "\n",
        "        scaler = scaler_target\n",
        "        y_test = y_test_target[:-24]\n",
        "        sqm = sqm_target\n",
        "\n",
        "\n",
        "        if mode == 'BASE_target':\n",
        "            model = createModel_BASE_target()\n",
        "            fit_model_plus_stats_BASE(model, x_train_target, y_train_target, 'target', type_)\n",
        "\n",
        "        elif mode == 'TL1_target':\n",
        "\n",
        "            model = createModel_TL1('target')\n",
        "            fit_model_plus_stats_TL1(model, x_train_target, y_train_target, 'target', type_)\n",
        "\n",
        "        elif mode == 'TL2_target':\n",
        "\n",
        "            model = createModel_TL2('target')\n",
        "            fit_model_plus_stats_TL2(model, x_train_target, y_train_target, 'target', type_)\n",
        "\n",
        "        elif mode == 'TL3_target':\n",
        "\n",
        "            model = createModel_TL3('target')\n",
        "            fit_model_plus_stats_TL3(model, x_train_target, y_train_target, 'target', type_)\n",
        "\n",
        "        predictions_scaled = predict_model(model, x_test_target)\n",
        "\n",
        "    elif mode == 'BASE_target_2' or mode == 'TL1_target_2' or mode == 'TL2_target_2' or mode == 'TL3_target_2':\n",
        "\n",
        "        scaler = scaler_target_2\n",
        "        y_test = y_test_target_2[:-24]\n",
        "        sqm = sqm_target_2\n",
        "\n",
        "        if mode == 'BASE_target_2':\n",
        "            model = createModel_BASE_target_2()\n",
        "            fit_model_plus_stats_BASE(model, x_train_target_2, y_train_target_2, 'target_2', type_)\n",
        "\n",
        "        elif mode == 'TL1_target_2':\n",
        "\n",
        "            model = createModel_TL1('target_2')\n",
        "            fit_model_plus_stats_TL1(model, x_train_target_2, y_train_target_2, 'target_2', type_)\n",
        "\n",
        "        elif mode == 'TL2_target_2':\n",
        "\n",
        "            model = createModel_TL2('target_2')\n",
        "            fit_model_plus_stats_TL2(model, x_train_target_2, y_train_target_2, 'target_2', type_)\n",
        "\n",
        "        elif mode == 'TL3_target_2':\n",
        "\n",
        "            model = createModel_TL3('target_2')\n",
        "            fit_model_plus_stats_TL3(model, x_train_target_2, y_train_target_2, 'target_2', type_)\n",
        "\n",
        "        predictions_scaled = predict_model(model, x_test_target_2)\n",
        "\n",
        "    predictions_scaled = predictions_scaled[:-24]\n",
        "\n",
        "    rmse_scaled = np.sqrt(mean_squared_error(y_test, predictions_scaled))\n",
        "    mae_scaled = mean_absolute_error(y_test, predictions_scaled)\n",
        "    mape_scaled = mean_absolute_percentage_error(y_test, predictions_scaled)\n",
        "    r2_score_scaled = r2_score(y_test, predictions_scaled)\n",
        "\n",
        "    print('Test Scaled RMSE: {}'.format(rmse_scaled))\n",
        "    print('Test Scaled MAE: {}'.format(mae_scaled))\n",
        "    print('Test Scaled R2 Score: ',r2_score_scaled)\n",
        "    print('Test Scaled MAPE: ',mape_scaled)\n",
        "\n",
        "    print()\n",
        "\n",
        "    predictions_unscaled = (scaler.inverse_transform(predictions_scaled))*(sqm/100)\n",
        "    true_measurements_unscaled = (scaler.inverse_transform(y_test))*(sqm/100)\n",
        "\n",
        "    rmse_unscaled = np.sqrt(mean_squared_error(true_measurements_unscaled, predictions_unscaled))\n",
        "    mae_unscaled = mean_absolute_error(true_measurements_unscaled, predictions_unscaled)\n",
        "    mape_unscaled = mean_absolute_percentage_error(true_measurements_unscaled, predictions_unscaled)\n",
        "    r2_score_unscaled = r2_score(true_measurements_unscaled, predictions_unscaled)*100\n",
        "\n",
        "    metrics_unscaled.append(rmse_unscaled)\n",
        "    metrics_unscaled.append(mae_unscaled)\n",
        "    metrics_unscaled.append(mape_unscaled)\n",
        "    metrics_unscaled.append(r2_score_unscaled)\n",
        "\n",
        "    print('Test Unscaled RMSE: {}'.format(rmse_unscaled))\n",
        "    print('Test Unscaled MAE: {}'.format(mae_unscaled))\n",
        "    print('Test Unscaled R2 Score: ',r2_score_unscaled)\n",
        "    print('Test Unscaled MAPE: ',mape_unscaled)\n",
        "\n",
        "    return predictions_unscaled, metrics_unscaled\n",
        "\n",
        "\n",
        "#------------- MAKE PREDICTIONS FOR 12M DATASETS ---------------#\n",
        "\n",
        "with open('Domain sqms/sqm_source.json', 'r') as f:\n",
        "    sqm_source = json.load(f)\n",
        "\n",
        "with open('Domain sqms/sqm_target.json', 'r') as f:\n",
        "    sqm_target = json.load(f)\n",
        "\n",
        "with open('Domain sqms/sqm_target_2.json', 'r') as f:\n",
        "    sqm_target_2 = json.load(f)\n",
        "\n",
        "\n",
        "scaler_source = joblib.load('Scalers/12M/source_scaler.pkl')\n",
        "scaler_target = joblib.load('Scalers/12M/target_scaler.pkl')\n",
        "scaler_target_2 = joblib.load('Scalers/12M/target_2_scaler.pkl')\n",
        "\n",
        "\n",
        "x_train_source = np.load('Data/Preprocessed data/12M/source_train_x.npy')\n",
        "y_train_source = np.load('Data/Preprocessed data/12M/source_train_y.npy')\n",
        "x_test_source = np.load('Data/Preprocessed data/12M/source_test_x.npy')\n",
        "y_test_source = np.load('Data/Preprocessed data/12M/source_test_y.npy')\n",
        "x_train_target = np.load('Data/Preprocessed data/12M/target_train_x.npy')\n",
        "y_train_target = np.load('Data/Preprocessed data/12M/target_train_y.npy')\n",
        "x_test_target = np.load('Data/Preprocessed data/12M/target_test_x.npy')\n",
        "y_test_target = np.load('Data/Preprocessed data/12M/target_test_y.npy')\n",
        "x_train_target_2 = np.load('Data/Preprocessed data/12M/target_2_train_x.npy')\n",
        "y_train_target_2 = np.load('Data/Preprocessed data/12M/target_2_train_y.npy')\n",
        "x_test_target_2 = np.load('Data/Preprocessed data/12M/target_2_test_x.npy')\n",
        "y_test_target_2 = np.load('Data/Preprocessed data/12M/target_2_test_y.npy')\n",
        "\n",
        "\n",
        "with open('Models/CNN/12M/Tuned Hyperparameters/source_params.json', 'r') as f:\n",
        "    source_params = json.load(f)\n",
        "\n",
        "with open('Models/CNN/12M/Tuned Hyperparameters/target_params.json', 'r') as f1:\n",
        "    target_params = json.load(f1)\n",
        "\n",
        "with open('Models/CNN/12M/Tuned Hyperparameters/target_2_params.json', 'r') as f2:\n",
        "    target_2_params = json.load(f2)\n",
        "\n",
        "\n",
        "#BASE MODEL PREDICTIONS FOR SOURCE DOMAIN DATASET\n",
        "\n",
        "predictions_base_source, metrics_base_source = model_fit_predict_results('BASE_source', '12M')\n",
        "\n",
        "np.save('Models/CNN/12M/Predictions/predictions_base_source.npy', predictions_base_source)\n",
        "with open('Models/CNN/12M/Metrics/metrics_base_source.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_base_source, file)\n",
        "\n",
        "\n",
        "#BASE MODEL PREDICTIONS FOR TARGET DOMAIN DATASET\n",
        "\n",
        "predictions_base_target, metrics_base_target = model_fit_predict_results('BASE_target', '12M')\n",
        "\n",
        "np.save('Models/CNN/12M/Predictions/predictions_base_target.npy', predictions_base_target)\n",
        "with open('Models/CNN/12M/Metrics/metrics_base_target.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_base_target, file)\n",
        "\n",
        "#BASE MODEL PREDICTIONS FOR TARGET DOMAIN 2 DATASET\n",
        "\n",
        "predictions_base_target_2, metrics_base_target_2 = model_fit_predict_results('BASE_target_2', '12M')\n",
        "\n",
        "np.save('Models/CNN/12M/Predictions/predictions_base_target_2.npy', predictions_base_target_2)\n",
        "with open('Models/CNN/12M/Metrics/metrics_base_target_2.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_base_target_2, file)\n",
        "\n",
        "#TL1 technique\n",
        "\n",
        "predictions_TL1_target, metrics_TL1_target = model_fit_predict_results('TL1_target', '12M')\n",
        "\n",
        "np.save('Models/CNN/12M/Predictions/predictions_TL1_target.npy', predictions_TL1_target)\n",
        "with open('Models/CNN/12M/Metrics/metrics_TL1_target.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_TL1_target, file)\n",
        "\n",
        "predictions_TL1_target_2, metrics_TL1_target_2 = model_fit_predict_results('TL1_target_2', '12M')\n",
        "\n",
        "np.save('Models/CNN/12M/Predictions/predictions_TL1_target_2.npy', predictions_TL1_target_2)\n",
        "with open('Models/CNN/12M/Metrics/metrics_TL1_target_2.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_TL1_target_2, file)\n",
        "\n",
        "\n",
        "#TL2 technique\n",
        "\n",
        "predictions_TL2_target, metrics_TL2_target = model_fit_predict_results('TL2_target', '12M')\n",
        "\n",
        "np.save('Models/CNN/12M/Predictions/predictions_TL2_target.npy', predictions_TL2_target)\n",
        "with open('Models/CNN/12M/Metrics/metrics_TL2_target.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_TL2_target, file)\n",
        "\n",
        "predictions_TL2_target_2, metrics_TL2_target_2 = model_fit_predict_results('TL2_target_2', '12M')\n",
        "\n",
        "np.save('Models/CNN/12M/Predictions/predictions_TL2_target_2.npy', predictions_TL2_target_2)\n",
        "with open('Models/CNN/12M/Metrics/metrics_TL2_target_2.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_TL2_target_2, file)\n",
        "\n",
        "\n",
        "#TL3 technique\n",
        "\n",
        "predictions_TL3_target, metrics_TL3_target = model_fit_predict_results('TL3_target', '12M')\n",
        "\n",
        "np.save('Models/CNN/12M/Predictions/predictions_TL3_target.npy', predictions_TL3_target)\n",
        "with open('Models/CNN/12M/Metrics/metrics_TL3_target.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_TL3_target, file)\n",
        "\n",
        "predictions_TL3_target_2, metrics_TL3_target_2 = model_fit_predict_results('TL3_target_2', '12M')\n",
        "\n",
        "np.save('Models/CNN/12M/Predictions/predictions_TL3_target_2.npy', predictions_TL3_target_2)\n",
        "with open('Models/CNN/12M/Metrics/metrics_TL3_target_2.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_TL3_target_2, file)\n",
        "\n",
        "\n",
        "#------------- MAKE PREDICTIONS FOR 6M DATASETS ---------------#\n",
        "\n",
        "\n",
        "with open('Domain sqms/sqm_target.json', 'r') as f:\n",
        "    sqm_target = json.load(f)\n",
        "\n",
        "\n",
        "scaler_target = joblib.load('Scalers/6M/target_scaler.pkl')\n",
        "\n",
        "\n",
        "x_train_target = np.load('Data/Preprocessed data/6M/target_train_x.npy')\n",
        "y_train_target = np.load('Data/Preprocessed data/6M/target_train_y.npy')\n",
        "x_test_target = np.load('Data/Preprocessed data/6M/target_test_x.npy')\n",
        "y_test_target = np.load('Data/Preprocessed data/6M/target_test_y.npy')\n",
        "\n",
        "\n",
        "with open('Models/CNN/6M/Tuned Hyperparameters/target_params.json', 'r') as f1:\n",
        "    target_params = json.load(f1)\n",
        "\n",
        "\n",
        "#BASE MODEL PREDICTIONS FOR TARGET DOMAIN DATASET\n",
        "\n",
        "predictions_base_target, metrics_base_target = model_fit_predict_results('BASE_target', '6M')\n",
        "\n",
        "np.save('Models/CNN/6M/Predictions/predictions_base_target.npy', predictions_base_target)\n",
        "with open('Models/CNN/6M/Metrics/metrics_base_target.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_base_target, file)\n",
        "\n",
        "\n",
        "#TL1 technique\n",
        "\n",
        "predictions_TL1_target, metrics_TL1_target = model_fit_predict_results('TL1_target', '6M')\n",
        "\n",
        "np.save('Models/CNN/6M/Predictions/predictions_TL1_target.npy', predictions_TL1_target)\n",
        "with open('Models/CNN/6M/Metrics/metrics_TL1_target.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_TL1_target, file)\n",
        "\n",
        "\n",
        "#TL2 technique\n",
        "\n",
        "predictions_TL2_target, metrics_TL2_target = model_fit_predict_results('TL2_target', '6M')\n",
        "\n",
        "np.save('Models/CNN/6M/Predictions/predictions_TL2_target.npy', predictions_TL2_target)\n",
        "with open('Models/CNN/6M/Metrics/metrics_TL2_target.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_TL2_target, file)\n",
        "\n",
        "\n",
        "#TL3 technique\n",
        "\n",
        "predictions_TL3_target, metrics_TL3_target = model_fit_predict_results('TL3_target', '6M')\n",
        "\n",
        "np.save('Models/CNN/6M/Predictions/predictions_TL3_target.npy', predictions_TL3_target)\n",
        "with open('Models/CNN/6M/Metrics/metrics_TL3_target.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_TL3_target, file)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
