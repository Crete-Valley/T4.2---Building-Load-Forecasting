{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "SMELiZ3evrTu",
        "outputId": "59d9182e-1fc0-4e85-b47a-7c6f1fd6e3df"
      },
      "outputs": [],
      "source": [
        "!pip install joblib\n",
        "!pip install pykalman\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import json\n",
        "import math\n",
        "import pykalman\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#read data files\n",
        "\n",
        "data_source = pd.read_pickle('Data/Final data/data_source.pkl')\n",
        "data_target = pd.read_pickle('Data/Final data/data_target.pkl')\n",
        "data_target_2 = pd.read_pickle('Data/Final data/data_target_2.pkl')\n",
        "\n",
        "\n",
        "#Function that checks for NaN values in each column\n",
        "\n",
        "def check_nan_intervals(df,mode):\n",
        "    for column in df.columns:\n",
        "\n",
        "        # Get a boolean series indicating where NaN values are\n",
        "        nan_series = df[column].isna()\n",
        "\n",
        "        # Find the indices where NaN values start and end\n",
        "        nan_starts = nan_series[(nan_series != nan_series.shift(1)) & nan_series].index\n",
        "        nan_ends = nan_series[(nan_series != nan_series.shift(-1)) & nan_series].index\n",
        "\n",
        "        # Print the intervals\n",
        "        longest_NaN_streak = 0\n",
        "        for start, end in zip(nan_starts, nan_ends):\n",
        "            if (longest_NaN_streak < end-start+1):\n",
        "                longest_NaN_streak = end-start+1\n",
        "            if (mode == 'NaN ANALYTICS'):\n",
        "              print(f\"Column: {column}\")\n",
        "              print(f\"NaN values from row {start} to {end}\")\n",
        "\n",
        "        # Check if there are trailing NaNs till the end of the dataframe\n",
        "        if nan_series.iloc[-1]:\n",
        "            if (longest_NaN_streak < len(df) - nan_starts[-1]):\n",
        "              longest_NaN_streak = len(df) - nan_starts[-1]\n",
        "            if (mode == 'NaN ANALYTICS'):\n",
        "              print(f\"NaN values from row {nan_starts[-1]} to the end\")\n",
        "\n",
        "        if (mode == 'NaN STREAK'):\n",
        "            if longest_NaN_streak > 0:\n",
        "                print(f\"Column: {column}\")\n",
        "                print(longest_NaN_streak)\n",
        "        # Extra line for better readability\n",
        "        print()\n",
        "\n",
        "\n",
        "#Function that imputates NaN values in a dataframe's column based on NaN values spread\n",
        "\n",
        "def check_longest_nan_streak_and_imputate(df,dataset):\n",
        "    for column in df.columns:\n",
        "\n",
        "        if (column != 'Electrical_Consumption' and dataset == 'source'):\n",
        "            print(f\"Column: {column}\")\n",
        "\n",
        "            # Get a boolean series indicating where NaN values are\n",
        "            nan_series = df[column].isna()\n",
        "\n",
        "            # Find the indices where NaN values start and end\n",
        "            nan_starts = nan_series[(nan_series != nan_series.shift(1)) & nan_series].index\n",
        "            nan_ends = nan_series[(nan_series != nan_series.shift(-1)) & nan_series].index\n",
        "\n",
        "            # Print the intervals\n",
        "            longest_NaN_streak = 0\n",
        "            for start, end in zip(nan_starts, nan_ends):\n",
        "                if (longest_NaN_streak < end - start + 1):\n",
        "                    longest_NaN_streak = end - start + 1\n",
        "\n",
        "            # Check if there are trailing NaNs till the end of the dataframe\n",
        "            if nan_series.iloc[-1]:\n",
        "                if (longest_NaN_streak < len(df) - nan_starts[-1]+1):\n",
        "                  longest_NaN_streak = len(df) - nan_starts[-1]+1\n",
        "\n",
        "            print(longest_NaN_streak)\n",
        "\n",
        "            if longest_NaN_streak > 0:\n",
        "\n",
        "                mov_avg_window = longest_NaN_streak + 2\n",
        "\n",
        "                col_data = df[column].copy()\n",
        "\n",
        "                # Apply moving average imputation\n",
        "                imputed_col = col_data.rolling(window=mov_avg_window, min_periods=1).mean()\n",
        "\n",
        "                # Fill NaN values with the computed moving average\n",
        "                col_data.fillna(imputed_col, inplace=True)\n",
        "\n",
        "                col_data.fillna(method='bfill', inplace=True)\n",
        "\n",
        "                # Update the original DataFrame with the imputed values\n",
        "                df[column] = col_data\n",
        "\n",
        "                print(f\"Column : {column} is imputated\")\n",
        "\n",
        "                # Extra line for better readability\n",
        "                print()\n",
        "\n",
        "        elif (column == 'Electrical_Consumption' and (dataset == 'source' or dataset == 'target')):\n",
        "            print(f\"Column: {column}\")\n",
        "\n",
        "            # Get a boolean series indicating where NaN values are\n",
        "            nan_series = df[column].isna()\n",
        "\n",
        "            # Find the indices where NaN values start and end\n",
        "            nan_starts = nan_series[(nan_series != nan_series.shift(1)) & nan_series].index\n",
        "            nan_ends = nan_series[(nan_series != nan_series.shift(-1)) & nan_series].index\n",
        "\n",
        "            # Print the intervals\n",
        "            longest_NaN_streak = 0\n",
        "            for start, end in zip(nan_starts, nan_ends):\n",
        "                if (longest_NaN_streak < end - start + 1):\n",
        "                    longest_NaN_streak = end - start + 1\n",
        "\n",
        "            # Check if there are trailing NaNs till the end of the dataframe\n",
        "            if nan_series.iloc[-1]:\n",
        "                if (longest_NaN_streak < len(df) - nan_starts[-1]+1):\n",
        "                  longest_NaN_streak = len(df) - nan_starts[-1]+1\n",
        "\n",
        "            print(longest_NaN_streak)\n",
        "\n",
        "            if longest_NaN_streak > 0:\n",
        "                for i in range(len(df)):\n",
        "                    if pd.isnull(df.at[i, column]):\n",
        "                        value = 'NaN'\n",
        "                        back = i\n",
        "                        front = i\n",
        "                        while (value != 'Imputated'):\n",
        "                            if (front+24 < len(df)):\n",
        "                                front = front+24\n",
        "                                if pd.notnull(df.at[front, column]):\n",
        "                                    df.at[i, column] = df.at[front, column]\n",
        "                                    value = 'Imputated'\n",
        "                            elif (back-24 > -1):\n",
        "                                back = back-24\n",
        "                                if pd.notnull(df.at[back, column]):\n",
        "                                    df.at[i, column] = df.at[back, column]\n",
        "                                    value = 'Imputated'\n",
        "                print(f\"Column : {column} is imputated\")\n",
        "\n",
        "                # Extra line for better readability\n",
        "                print()\n",
        "\n",
        "        elif (column != 'CloudCoverage' and (dataset == 'target' or dataset == 'target_2')):\n",
        "            print(f\"Column: {column}\")\n",
        "\n",
        "            # Get a boolean series indicating where NaN values are\n",
        "            nan_series = df[column].isna()\n",
        "\n",
        "            # Find the indices where NaN values start and end\n",
        "            nan_starts = nan_series[(nan_series != nan_series.shift(1)) & nan_series].index\n",
        "            nan_ends = nan_series[(nan_series != nan_series.shift(-1)) & nan_series].index\n",
        "\n",
        "            # Print the intervals\n",
        "            longest_NaN_streak = 0\n",
        "            for start, end in zip(nan_starts, nan_ends):\n",
        "                if (longest_NaN_streak < end - start + 1):\n",
        "                    longest_NaN_streak = end - start + 1\n",
        "\n",
        "            # Check if there are trailing NaNs till the end of the dataframe\n",
        "            if nan_series.iloc[-1]:\n",
        "                if (longest_NaN_streak < len(df) - nan_starts[-1]+1):\n",
        "                  longest_NaN_streak = len(df) - nan_starts[-1]+1\n",
        "\n",
        "            print(longest_NaN_streak)\n",
        "\n",
        "            if longest_NaN_streak > 0:\n",
        "\n",
        "                mov_avg_window = longest_NaN_streak + 2\n",
        "\n",
        "                col_data = df[column].copy()\n",
        "\n",
        "                # Apply moving average imputation\n",
        "                imputed_col = col_data.rolling(window=mov_avg_window, min_periods=1).mean()\n",
        "\n",
        "                # Fill NaN values with the computed moving average\n",
        "                col_data.fillna(imputed_col, inplace=True)\n",
        "\n",
        "                col_data.fillna(method='bfill', inplace=True)\n",
        "\n",
        "                # Update the original DataFrame with the imputed values\n",
        "                df[column] = col_data\n",
        "\n",
        "                print(f\"Column : {column} is imputated\")\n",
        "\n",
        "                # Extra line for better readability\n",
        "                print()\n",
        "\n",
        "        elif (column == 'CloudCoverage' and (dataset == 'target' or dataset == 'target_2')):\n",
        "\n",
        "            df[column] = df[column].interpolate(method='spline', order=3)\n",
        "\n",
        "            print(f\"Column : {column} is imputated\")\n",
        "\n",
        "            # Extra line for better readability\n",
        "            print()\n",
        "\n",
        "\n",
        "#Extract month information from timestamp column and perform one-hot encoding\n",
        "\n",
        "def timestamp_month_processing(data):\n",
        "\n",
        "    data['timestamp'] = data['timestamp'].dt.hour\n",
        "\n",
        "    # Get the dummies for column 'month'\n",
        "    dummies = pd.get_dummies(data['month'], prefix='month')\n",
        "\n",
        "    # Find the original column position\n",
        "    col_position = data.columns.get_loc('month')\n",
        "\n",
        "    # Drop the original column 'month'\n",
        "    data.drop('month', axis=1, inplace=True)\n",
        "\n",
        "    # Insert the dummy columns at the original position\n",
        "    for i, col in enumerate(dummies.columns):\n",
        "        data.insert(col_position + i, col, dummies[col])\n",
        "\n",
        "    month_cols = [ 'month_1',\n",
        "                   'month_2',\n",
        "                   'month_3',\n",
        "                   'month_4',\n",
        "                   'month_5',\n",
        "                   'month_6',\n",
        "                   'month_7',\n",
        "                   'month_8',\n",
        "                   'month_9',\n",
        "                   'month_10',\n",
        "                   'month_11',\n",
        "                   'month_12']\n",
        "\n",
        "    for col in month_cols:\n",
        "        data[col] = data[col].astype(int)\n",
        "\n",
        "\n",
        "#train test split function\n",
        "\n",
        "def train_test_split(data):\n",
        "\n",
        "    data = data[cols]\n",
        "    train = data.iloc[:int(len(data)*split),:]\n",
        "    test = data.iloc[int(len(data)*split):,]\n",
        "    return train,test\n",
        "\n",
        "def strip_leading_zeros(x):\n",
        "    return int(x.lstrip('0')) if isinstance(x, str) else x\n",
        "\n",
        "#Function that creates the sequential input from the dataset provided\n",
        "\n",
        "def split_sequences(sequences, n_steps, n_outputs):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequences)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "        # check if we are beyond the dataset\n",
        "        if end_ix < len(sequences):\n",
        "            # gather input and output parts of the pattern\n",
        "\n",
        "            if (len(sequences.iloc[i:end_ix, :]) == 48 and len(sequences.iloc[end_ix:(end_ix+n_outputs), -1]) == 1):\n",
        "                seq_x = sequences.iloc[i:end_ix, :]\n",
        "                seq_y = sequences.iloc[end_ix:(end_ix+n_outputs), -1]\n",
        "            else :\n",
        "                #debugging prints\n",
        "                print(i)\n",
        "                print(sequences.iloc[i:end_ix, :])\n",
        "                print(sequences.iloc[end_ix:(end_ix+n_outputs), -1])\n",
        "            X.append(seq_x)\n",
        "            y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "#Function that makes sure each input sequence has the same shape\n",
        "\n",
        "def unique_shapes(x, y, lag_, n_features_, num_of_outputs_):\n",
        "    uniuqe_shapes = []\n",
        "    for k in range(len(x)):\n",
        "\n",
        "        if (x[k].shape == (lag_, n_features_)) & (y[k].shape == (num_of_outputs_,)):\n",
        "                uniuqe_shapes.append(k)\n",
        "\n",
        "    x = x[uniuqe_shapes]\n",
        "    y = y[uniuqe_shapes]\n",
        "    x = np.stack(x)\n",
        "    y = np.stack(y)\n",
        "    return x, y\n",
        "\n",
        "def scale_data(x_train, x_test, y_train, y_test):\n",
        "\n",
        "    # Reshape features for scaling\n",
        "    x_train_flat = x_train.reshape(-1, x_train.shape[-1])\n",
        "    x_test_flat = x_test.reshape(-1, x_test.shape[-1])\n",
        "\n",
        "    # Initialize scalers\n",
        "    X_scaler = MinMaxScaler()\n",
        "    Y_scaler = MinMaxScaler()\n",
        "\n",
        "    # Fit scalers on training data\n",
        "    X_scaler.fit(x_train_flat)\n",
        "    Y_scaler.fit(y_train)\n",
        "\n",
        "    # Scale features\n",
        "    x_train_scaled = X_scaler.transform(x_train_flat).reshape(x_train.shape)\n",
        "    x_test_scaled = X_scaler.transform(x_test_flat).reshape(x_test.shape)\n",
        "\n",
        "    # Scale target\n",
        "    y_train_scaled = Y_scaler.transform(y_train)\n",
        "    y_test_scaled = Y_scaler.transform(y_test)\n",
        "\n",
        "    return x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled, Y_scaler\n",
        "\n",
        "def adjust_value(val, mean, lower_bound, upper_bound):\n",
        "        if val < lower_bound:\n",
        "            return mean + (val - lower_bound)/2  # Scale up the value closer to the mean\n",
        "        elif val > upper_bound:\n",
        "            return mean + (val - upper_bound)/2  # Scale down the value closer to the mean\n",
        "        else:\n",
        "            return val\n",
        "\n",
        "\n",
        "def tweak_outliers(data, level, feature):\n",
        "    while level > 0:\n",
        "        for year in range (2016, 2018):\n",
        "            for month in range(1,13):\n",
        "                if month < 12:\n",
        "                    value_list = data[(data['timestamp'] >= datetime(year, month, 1, 0, 0, 0)) & (data['timestamp'] <= datetime(year,month+1, 1, 0, 0, 0))][feature]\n",
        "                    mean = value_list.mean()\n",
        "                    std_dev = value_list.std()\n",
        "                    lower_bound = mean - 2 * std_dev\n",
        "                    upper_bound = mean + 2 * std_dev\n",
        "                    for i in data.index:\n",
        "                        if data.loc[i, 'timestamp'] >= datetime(year, month, 1, 0, 0, 0) and data.loc[i, 'timestamp'] < datetime(year, month+1, 1, 0, 0, 0) :\n",
        "                            data.loc[i, feature] = adjust_value(data.loc[i, feature], mean, lower_bound, upper_bound)\n",
        "\n",
        "                elif month == 12:\n",
        "                    value_list = data[(data['timestamp'] >= datetime(year, month, 1, 0, 0, 0)) & (data['timestamp'] <= datetime(year+1, 1, 1, 0, 0, 0))][feature]\n",
        "                    mean = value_list.mean()\n",
        "                    std_dev = value_list.std()\n",
        "                    lower_bound = mean - 2 * std_dev\n",
        "                    upper_bound = mean + 2 * std_dev\n",
        "                    for i in data.index:\n",
        "                        if data.loc[i, 'timestamp'] >= datetime(year, month, 1, 0, 0, 0) and data.loc[i, 'timestamp'] < datetime(year+1, 1, 1, 0, 0, 0) :\n",
        "                            data.loc[i, feature] = adjust_value(data.loc[i, feature], mean, lower_bound, upper_bound)\n",
        "\n",
        "\n",
        "        level = level - 1\n",
        "\n",
        "def fix_nans(data, feature):\n",
        "    for i in range(len(data)):\n",
        "        if math.isnan(data.loc[i, feature]):\n",
        "            if i < 8784:\n",
        "                data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]+pd.DateOffset(years= 1)][feature].item()\n",
        "            elif i >= 8784:\n",
        "                data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]-pd.DateOffset(years= 1)][feature].item()\n",
        "\n",
        "            if math.isnan(data.loc[i, feature]):\n",
        "                if i+24 < len(data_source):\n",
        "                    data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]+pd.DateOffset(days= 1)][feature].item()\n",
        "\n",
        "                if i-24 >= 0 and math.isnan(data.loc[i, feature]):\n",
        "                    data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]-pd.DateOffset(days= 1)][feature].item()\n",
        "\n",
        "                if math.isnan(data.loc[i, feature]):\n",
        "                    if i+48 < len(data_source):\n",
        "                        data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]+pd.DateOffset(days= 2)][feature].item()\n",
        "\n",
        "                    if i-48 >= 0 and math.isnan(data.loc[i, feature]):\n",
        "                        data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]-pd.DateOffset(days= 2)][feature].item()\n",
        "\n",
        "                    if math.isnan(data.loc[i, feature]):\n",
        "                        if i+72 < len(data_source):\n",
        "                            data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]+pd.DateOffset(days= 3)][feature].item()\n",
        "\n",
        "                        if i-72 >= 0 and math.isnan(data.loc[i, feature]):\n",
        "                            data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]-pd.DateOffset(days= 3)][feature].item()\n",
        "\n",
        "                        if math.isnan(data.loc[i, feature]):\n",
        "                            if i+96 < len(data_source):\n",
        "                                data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]+pd.DateOffset(days= 4)][feature].item()\n",
        "\n",
        "                            if i-96 >= 0 and math.isnan(data.loc[i, feature]):\n",
        "                                data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]-pd.DateOffset(days= 4)][feature].item()\n",
        "\n",
        "                            if math.isnan(data.loc[i, feature]):\n",
        "                                if i+120 < len(data_source):\n",
        "                                    data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]+pd.DateOffset(days= 5)][feature].item()\n",
        "\n",
        "                                if i-120 >= 0 and math.isnan(data.loc[i, feature]):\n",
        "                                    data.loc[i, feature] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]-pd.DateOffset(days= 5)][feature].item()\n",
        "\n",
        "\n",
        "\n",
        "def plot_feature(data, feature, domain):\n",
        "    plt.figure(figsize=(15, 9))\n",
        "    plt.plot(data['timestamp'], data[feature], label='{} Dataset'.format(domain), color='green', linestyle=':')\n",
        "    plt.xlabel('Timestamp')\n",
        "    plt.ylabel(feature)\n",
        "    plt.title('Visualization of {} across {} dataset'.format(feature, domain))\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#convert timestamp to a datetime format\n",
        "\n",
        "data_source['timestamp'] = pd.to_datetime(data_source['timestamp'])\n",
        "data_target['timestamp'] = pd.to_datetime(data_target['timestamp'])\n",
        "data_target_2['timestamp'] = pd.to_datetime(data_target_2['timestamp'])\n",
        "\n",
        "\n",
        "#create a month column for each dataset\n",
        "\n",
        "month_column = data_source['timestamp'].dt.month\n",
        "data_source.insert(1, 'month', month_column)\n",
        "data_source['month'] = data_source['month'].apply(strip_leading_zeros)\n",
        "\n",
        "month_column = data_target['timestamp'].dt.month\n",
        "data_target.insert(1, 'month', month_column)\n",
        "data_target['month'] = data_target['month'].apply(strip_leading_zeros)\n",
        "\n",
        "month_column = data_target_2['timestamp'].dt.month\n",
        "data_target_2.insert(1, 'month', month_column)\n",
        "data_target_2['month'] = data_target_2['month'].apply(strip_leading_zeros)\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------#\n",
        "#--------------------------------process the source dataset--------------------------------------------#\n",
        "#--------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "print(\"NaN value streaks in data source\")\n",
        "print()\n",
        "\n",
        "\n",
        "check_nan_intervals(data_source, 'NaN STREAK')\n",
        "\n",
        "\n",
        "#plot before processing\n",
        "\n",
        "plot_feature(data_source, \"Electrical_Consumption\", \"source\")\n",
        "\n",
        "\n",
        "print(\"Filling NaN values in data source\")\n",
        "print()\n",
        "\n",
        "for i in range(len(data_source)):\n",
        "    if data_source.loc[i, 'Electrical_Consumption'] < 13:\n",
        "        data_source.loc[i, 'Electrical_Consumption'] = data_source[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]-pd.DateOffset(years= 1)]['Electrical_Consumption'].item()\n",
        "    elif data_source.loc[i, 'Electrical_Consumption'] > 65:\n",
        "        data_source.loc[i, 'Electrical_Consumption'] = (data_source.loc[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]-pd.DateOffset(days= 1)]['Electrical_Consumption'].item() + data_source.loc[data_source[\"timestamp\"]==data_source.loc[i, \"timestamp\"]+pd.DateOffset(days= 1)]['Electrical_Consumption'].item())/2\n",
        "\n",
        "\n",
        "\n",
        "fix_nans(data_source, \"cloudCoverage\")\n",
        "data_source[\"cloudCoverage\"] = data_source[\"cloudCoverage\"].interpolate(method='spline', order=3)\n",
        "fix_nans(data_source, \"airTemperature\")\n",
        "fix_nans(data_source, \"dewTemperature\")\n",
        "fix_nans(data_source, \"windDirection\")\n",
        "fix_nans(data_source, \"windSpeed\")\n",
        "\n",
        "print(\"Fixing outliers in data source\")\n",
        "print()\n",
        "\n",
        "tweak_outliers(data_source, 1, \"Electrical_Consumption\")\n",
        "#tweak_outliers(data_target, 1, \"cloudCoverage\")\n",
        "#tweak_outliers(data_target, 1, \"airTemperature\")\n",
        "#tweak_outliers(data_target, 1, \"dewTemperature\")\n",
        "#tweak_outliers(data_target, 1, \"windDirection\")\n",
        "#tweak_outliers(data_target, 1, \"windSpeed\")\n",
        "\n",
        "\n",
        "\n",
        "plot_feature(data_source, \"Electrical_Consumption\", \"source\")\n",
        "\n",
        "check_nan_intervals(data_source, 'NaN STREAK')\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------#\n",
        "#--------------------------------process the target dataset----------------------------------------------#\n",
        "#--------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "print(\"NaN value streaks in data target\")\n",
        "print()\n",
        "\n",
        "check_nan_intervals(data_target, 'NaN STREAK')\n",
        "\n",
        "\n",
        "#plot before processing\n",
        "\n",
        "plot_feature(data_target, \"Electrical_Consumption\", \"target\")\n",
        "\n",
        "print(\"Filling NaN values in data target\")\n",
        "print()\n",
        "\n",
        "\n",
        "fix_nans(data_target, \"Electrical_Consumption\")\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(data_target)):\n",
        "    if data_target.loc[i,'timestamp'] >= datetime(2016, 1, 4, 0, 0, 0) and data_target.loc[i,'timestamp'] <= datetime(2017, 12, 27, 0, 0, 0):\n",
        "        p1 = data_target[data_target['timestamp'] == data_target.loc[i, 'timestamp']-pd.DateOffset(days= 1)]['Electrical_Consumption'].item()\n",
        "        p2 = data_target[data_target['timestamp'] == data_target.loc[i, 'timestamp']-pd.DateOffset(days= 2)]['Electrical_Consumption'].item()\n",
        "        f1 = data_target[data_target['timestamp'] == data_target.loc[i, 'timestamp']+pd.DateOffset(days= 1)]['Electrical_Consumption'].item()\n",
        "        f2 =  data_target[data_target['timestamp'] == data_target.loc[i, 'timestamp']+pd.DateOffset(days= 2)]['Electrical_Consumption'].item()\n",
        "\n",
        "        if data_target.loc[i,'Electrical_Consumption'] >= max(p1,p2,f1,f2) + 8 or data_target.loc[i,'Electrical_Consumption'] <= min(p1,p2,f1,f2) - 8:\n",
        "            data_target.loc[i,'Electrical_Consumption'] = (p1+p2+f1+f2)/4\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(data_target)):\n",
        "    if data_target.loc[i,'timestamp'] <= datetime(2016, 1, 10, 0, 0, 0) or (data_target.loc[i,'timestamp'] >= datetime(2016, 5, 8, 0, 0, 0) and data_target.loc[i,'timestamp'] <= datetime(2016, 8, 15, 0, 0, 0)):\n",
        "        data_target.loc[i, 'Electrical_Consumption'] = data_target[data_target['timestamp'] == data_target.loc[i, 'timestamp']+pd.DateOffset(years= 1)]['Electrical_Consumption'].item()\n",
        "\n",
        "    if data_target.loc[i,'timestamp'] >= datetime(2017, 12, 15, 0, 0, 0):\n",
        "        data_target.loc[i, 'Electrical_Consumption'] = data_target[data_target['timestamp'] == data_target.loc[i, 'timestamp']-pd.DateOffset(years= 1)]['Electrical_Consumption'].item()\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(data_target)):\n",
        "    if data_target.loc[i,'Electrical_Consumption'] >= 120 or data_target.loc[i,'Electrical_Consumption'] <= 60:\n",
        "        p1 = data_target[data_target['timestamp'] == data_target.loc[i, 'timestamp']-pd.DateOffset(days= 1)]['Electrical_Consumption'].item()\n",
        "        p2 = data_target[data_target['timestamp'] == data_target.loc[i, 'timestamp']-pd.DateOffset(days= 2)]['Electrical_Consumption'].item()\n",
        "        f1 = data_target[data_target['timestamp'] == data_target.loc[i, 'timestamp']+pd.DateOffset(days= 1)]['Electrical_Consumption'].item()\n",
        "        f2 =  data_target[data_target['timestamp'] == data_target.loc[i, 'timestamp']+pd.DateOffset(days= 2)]['Electrical_Consumption'].item()\n",
        "\n",
        "        data_target.loc[i,'Electrical_Consumption'] = (p1+p2+f1+f2)/4\n",
        "\n",
        "\n",
        "\n",
        "fix_nans(data_target, \"cloudCoverage\")\n",
        "fix_nans(data_target, \"airTemperature\")\n",
        "fix_nans(data_target, \"dewTemperature\")\n",
        "fix_nans(data_target, \"windDirection\")\n",
        "fix_nans(data_target, \"windSpeed\")\n",
        "\n",
        "print(\"Fixing outliers in data target\")\n",
        "print()\n",
        "\n",
        "\n",
        "tweak_outliers(data_target, 1, \"Electrical_Consumption\")\n",
        "#tweak_outliers(data_target, 1, \"cloudCoverage\")\n",
        "#tweak_outliers(data_target, 1, \"airTemperature\")\n",
        "#tweak_outliers(data_target, 1, \"dewTemperature\")\n",
        "#tweak_outliers(data_target, 1, \"windDirection\")\n",
        "#tweak_outliers(data_target, 1, \"windSpeed\")\n",
        "\n",
        "\n",
        "#plot after the processing\n",
        "\n",
        "plot_feature(data_target, \"Electrical_Consumption\", \"target\")\n",
        "\n",
        "\n",
        "check_nan_intervals(data_target, 'NaN STREAK')\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------#\n",
        "#--------------------------------process the target 2 dataset----------------------------------------------#\n",
        "#--------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "print(\"NaN value streaks in data target 2\")\n",
        "print()\n",
        "\n",
        "check_nan_intervals(data_target_2, 'NaN STREAK')\n",
        "\n",
        "\n",
        "#plot before the processing\n",
        "\n",
        "plot_feature(data_target_2, \"Electrical_Consumption\", \"source\")\n",
        "\n",
        "\n",
        "print(\"Filling NaN values in data target 2\")\n",
        "print()\n",
        "\n",
        "for i in range(9860, 9864):\n",
        "    next_day_value = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[i, \"timestamp\"]+pd.DateOffset(days= 1)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "    data_target_2.loc[i, 'Electrical_Consumption'] = next_day_value\n",
        "\n",
        "for i in range(12860, 12865):\n",
        "    last_year_value = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[i, \"timestamp\"]-pd.DateOffset(years= 1)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "    data_target_2.loc[i, 'Electrical_Consumption'] = last_year_value\n",
        "\n",
        "\n",
        "\n",
        "#fixing NaN value on datapoint 10466\n",
        "data_target_2.loc[10139, 'Electrical_Consumption'] = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[10139, \"timestamp\"]+pd.DateOffset(days= 1)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "#fixing the outlier on datapoint 12859\n",
        "data_target_2.loc[10466, 'Electrical_Consumption'] = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[10466, \"timestamp\"]+pd.DateOffset(days= 1)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "for i in range(0, 3378):\n",
        "    data_target_2.loc[i, 'Electrical_Consumption'] = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[i, \"timestamp\"]+pd.DateOffset(years= 1)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "\n",
        "for i in range(len(data_target_2)):\n",
        "    if data_target_2.loc[i, 'Electrical_Consumption'] > 90 or data_target_2.loc[i, 'Electrical_Consumption'] < 35:\n",
        "        if i > 8784 :\n",
        "            data_target_2.loc[i, 'Electrical_Consumption'] = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[i, \"timestamp\"]-pd.DateOffset(years= 1)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "            if data_target_2.loc[i, 'Electrical_Consumption'] > 90 or data_target_2.loc[i, 'Electrical_Consumption'] < 35:\n",
        "                data_target_2.loc[i, 'Electrical_Consumption'] = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[i, \"timestamp\"]-pd.DateOffset(days= 1)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "            if data_target_2.loc[i, 'Electrical_Consumption'] > 90 or data_target_2.loc[i, 'Electrical_Consumption'] < 35:\n",
        "                data_target_2.loc[i, 'Electrical_Consumption'] = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[i, \"timestamp\"]-pd.DateOffset(days= 2)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "            if data_target_2.loc[i, 'Electrical_Consumption'] > 90 or data_target_2.loc[i, 'Electrical_Consumption'] < 35:\n",
        "                data_target_2.loc[i, 'Electrical_Consumption'] = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[i, \"timestamp\"]-pd.DateOffset(days= 3)][\"Electrical_Consumption\"].item()\n",
        "        elif i < 8784:\n",
        "            data_target_2.loc[i, 'Electrical_Consumption'] = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[i, \"timestamp\"]+pd.DateOffset(years= 1)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "            if data_target_2.loc[i, 'Electrical_Consumption'] > 90 or data_target_2.loc[i, 'Electrical_Consumption'] < 35:\n",
        "                data_target_2.loc[i, 'Electrical_Consumption'] = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[i, \"timestamp\"]+pd.DateOffset(days= 1)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "            if data_target_2.loc[i, 'Electrical_Consumption'] > 90 or data_target_2.loc[i, 'Electrical_Consumption'] < 35:\n",
        "                data_target_2.loc[i, 'Electrical_Consumption'] = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[i, \"timestamp\"]+pd.DateOffset(days= 2)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "            if data_target_2.loc[i, 'Electrical_Consumption'] > 90 or data_target_2.loc[i, 'Electrical_Consumption'] < 35:\n",
        "                data_target_2.loc[i, 'Electrical_Consumption'] = data_target_2[data_target_2[\"timestamp\"]==data_target_2.loc[i, \"timestamp\"]+pd.DateOffset(days= 3)][\"Electrical_Consumption\"].item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fix_nans(data_target_2, \"cloudCoverage\")\n",
        "data_target_2[\"cloudCoverage\"] = data_target_2[\"cloudCoverage\"].interpolate(method='spline', order=3)\n",
        "fix_nans(data_target_2, \"airTemperature\")\n",
        "fix_nans(data_target_2, \"dewTemperature\")\n",
        "fix_nans(data_target_2, \"windDirection\")\n",
        "fix_nans(data_target_2, \"windSpeed\")\n",
        "\n",
        "print(\"Fixing outliers in data target 2\")\n",
        "print()\n",
        "\n",
        "tweak_outliers(data_target_2, 1, \"Electrical_Consumption\")\n",
        "#tweak_outliers(data_target_2, 1, \"cloudCoverage\")\n",
        "#tweak_outliers(data_target_2, 1, \"airTemperature\")\n",
        "#tweak_outliers(data_target_2, 1, \"dewTemperature\")\n",
        "#tweak_outliers(data_target_2, 1, \"windDirection\")\n",
        "#tweak_outliers(data_target_2, 1, \"windSpeed\")\n",
        "\n",
        "#plot after the processing\n",
        "\n",
        "plot_feature(data_target_2, \"Electrical_Consumption\", \"target_2\")\n",
        "\n",
        "\n",
        "check_nan_intervals(data_target_2, 'NaN STREAK')\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------#\n",
        "#-------------------------------------- Final preprocessing ---------------------------------------------#\n",
        "#--------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "#create a new column representing the relationship between 2 previous ones\n",
        "\n",
        "print(\"'month' one hot encoding, 'Electrical_Consumption/sqm' creation, sequence format creation and scaling for all datasets\")\n",
        "print()\n",
        "\n",
        "extra_column = data_source['Electrical_Consumption'].copy()\n",
        "extra_column2 = data_target['Electrical_Consumption'].copy()\n",
        "extra_column3 = data_target_2['Electrical_Consumption'].copy()\n",
        "\n",
        "sqm = data_source.at[1,'sqm']\n",
        "\n",
        "with open('Domain sqms/sqm_source.json', 'w') as f:\n",
        "    json.dump(sqm, f)\n",
        "\n",
        "sqm2 = data_target.at[1,'sqm']\n",
        "\n",
        "with open('Domain sqms/sqm_target.json', 'w') as f:\n",
        "    json.dump(sqm2, f)\n",
        "\n",
        "sqm3 = data_target_2.at[1,'sqm']\n",
        "\n",
        "with open('Domain sqms/sqm_target_2.json', 'w') as f:\n",
        "    json.dump(sqm3, f)\n",
        "\n",
        "for i in range(max(len(data_source),len(data_target),len(data_target_2))):\n",
        "    if i < len(data_source):\n",
        "        extra_column[i] = (extra_column[i]/sqm)*100\n",
        "\n",
        "    if i < len(data_target):\n",
        "        extra_column2[i] = (extra_column2[i]/sqm2)*100\n",
        "\n",
        "    if i < len(data_target_2):\n",
        "        extra_column3[i] = (extra_column3[i]/sqm3)*100\n",
        "\n",
        "data_source.insert(10, 'Electrical_Consumption/sqm', extra_column)\n",
        "data_target.insert(10, 'Electrical_Consumption/sqm', extra_column2)\n",
        "data_target_2.insert(10, 'Electrical_Consumption/sqm', extra_column3)\n",
        "\n",
        "data_source.pop('sqm')\n",
        "data_target.pop('sqm')\n",
        "data_target_2.pop('sqm')\n",
        "\n",
        "data_source.pop('Electrical_Consumption')\n",
        "data_target.pop('Electrical_Consumption')\n",
        "data_target_2.pop('Electrical_Consumption')\n",
        "\n",
        "cols = ['timestamp',\n",
        "        'month_1',\n",
        "        'month_2',\n",
        "        'month_3',\n",
        "        'month_4',\n",
        "        'month_5',\n",
        "        'month_6',\n",
        "        'month_7',\n",
        "        'month_8',\n",
        "        'month_9',\n",
        "        'month_10',\n",
        "        'month_11',\n",
        "        'month_12',\n",
        "        'lat',\n",
        "        'lng',\n",
        "        'yearbuilt',\n",
        "        'airTemperature',\n",
        "        'cloudCoverage',\n",
        "        'dewTemperature',\n",
        "        'windDirection',\n",
        "        'windSpeed',\n",
        "        'Electrical_Consumption/sqm']\n",
        "\n",
        "\n",
        "\n",
        "# Splitting factor for training set and test set\n",
        "split = 0.8\n",
        "\n",
        "# Select the lag variable, the number of features (must be same with cols selected) and the horizon\n",
        "lag = 48  #lookbackwindow\n",
        "n_features = len(cols)\n",
        "num_of_outputs = 1\n",
        "\n",
        "split_starting_index_12M = data_source[data_source['timestamp'] == datetime(2016, 5, 1, 0, 0, 0)].index[0]\n",
        "split_ending_index_12M = data_source[data_source['timestamp'] == datetime(2017, 5, 1, 0, 0, 0)].index[0]\n",
        "\n",
        "split_starting_index_6M = data_source[data_source['timestamp'] == datetime(2016, 11, 1, 0, 0, 0)].index[0]\n",
        "split_ending_index_6M = data_source[data_source['timestamp'] == datetime(2017, 5, 1, 0, 0, 0)].index[0]\n",
        "\n",
        "timestamp_month_processing(data_source)\n",
        "timestamp_month_processing(data_target)\n",
        "timestamp_month_processing(data_target_2)\n",
        "\n",
        "for i in range(2):\n",
        "\n",
        "    if i==0:\n",
        "        data_target_final = data_target.iloc[split_starting_index_12M: split_ending_index_12M]\n",
        "        data_target_2_final = data_target_2.iloc[split_starting_index_12M: split_ending_index_12M]\n",
        "    elif i==1:\n",
        "        data_target_final = data_target.iloc[split_starting_index_6M: split_ending_index_6M]\n",
        "        data_target_2_final = data_target_2.iloc[split_starting_index_6M: split_ending_index_6M]\n",
        "\n",
        "    #train-test split the data\n",
        "    train_source, test_source =  train_test_split(data_source)\n",
        "    train_target, test_target =  train_test_split(data_target_final)\n",
        "    train_target_2, test_target_2 =  train_test_split(data_target_2_final)\n",
        "\n",
        "\n",
        "    # Create the sequential input : x(total number of sequences, lag, features), y(total number of sequences) for each dataset\n",
        "    x_train_source_unscaled, y_train_source_unscaled = split_sequences(train_source, n_steps=lag, n_outputs=num_of_outputs)\n",
        "    x_test_source_unscaled, y_test_source_unscaled = split_sequences(test_source, n_steps=lag, n_outputs=num_of_outputs)\n",
        "\n",
        "    x_train_target_unscaled, y_train_target_unscaled = split_sequences(train_target, n_steps=lag, n_outputs=num_of_outputs)\n",
        "    x_test_target_unscaled, y_test_target_unscaled = split_sequences(test_target, n_steps=lag, n_outputs=num_of_outputs)\n",
        "\n",
        "    x_train_target_2_unscaled, y_train_target_2_unscaled = split_sequences(train_target_2, n_steps=lag, n_outputs=num_of_outputs)\n",
        "    x_test_target_2_unscaled, y_test_target_2_unscaled = split_sequences(test_target_2, n_steps=lag, n_outputs=num_of_outputs)\n",
        "\n",
        "\n",
        "\n",
        "    #source domain scaling\n",
        "\n",
        "    if i == 0:\n",
        "\n",
        "        #source domain scaling\n",
        "\n",
        "        x_train_source, x_test_source, y_train_source, y_test_source, Y_scaler_source = scale_data(x_train_source_unscaled, x_test_source_unscaled, y_train_source_unscaled, y_test_source_unscaled)\n",
        "\n",
        "        joblib.dump(Y_scaler_source, 'Scalers/12M/source_scaler.pkl')\n",
        "\n",
        "\n",
        "        #target domain scaling\n",
        "\n",
        "        x_train_target, x_test_target, y_train_target, y_test_target, Y_scaler_target = scale_data(x_train_target_unscaled, x_test_target_unscaled, y_train_target_unscaled, y_test_target_unscaled)\n",
        "\n",
        "        joblib.dump(Y_scaler_target, 'Scalers/12M/target_scaler.pkl')\n",
        "\n",
        "        #target 2 domain scaling\n",
        "\n",
        "        x_train_target_2, x_test_target_2, y_train_target_2, y_test_target_2, Y_scaler_target_2 = scale_data(x_train_target_2_unscaled, x_test_target_2_unscaled, y_train_target_2_unscaled, y_test_target_2_unscaled)\n",
        "\n",
        "        joblib.dump(Y_scaler_target_2, 'Scalers/12M/target_2_scaler.pkl')\n",
        "\n",
        "    elif i == 1:\n",
        "\n",
        "        #source domain scaling\n",
        "\n",
        "        x_train_source, x_test_source, y_train_source, y_test_source, Y_scaler_source = scale_data(x_train_source_unscaled, x_test_source_unscaled, y_train_source_unscaled, y_test_source_unscaled)\n",
        "\n",
        "        joblib.dump(Y_scaler_source, 'Scalers/6M/source_scaler.pkl')\n",
        "\n",
        "\n",
        "        #target domain scaling\n",
        "\n",
        "        x_train_target, x_test_target, y_train_target, y_test_target, Y_scaler_target = scale_data(x_train_target_unscaled, x_test_target_unscaled, y_train_target_unscaled, y_test_target_unscaled)\n",
        "\n",
        "        joblib.dump(Y_scaler_target, 'Scalers/6M/target_scaler.pkl')\n",
        "\n",
        "        #target 2 domain scaling\n",
        "\n",
        "        x_train_target_2, x_test_target_2, y_train_target_2, y_test_target_2, Y_scaler_target_2 = scale_data(x_train_target_2_unscaled, x_test_target_2_unscaled, y_train_target_2_unscaled, y_test_target_2_unscaled)\n",
        "\n",
        "        joblib.dump(Y_scaler_target_2, 'Scalers/6M/target_2_scaler.pkl')\n",
        "\n",
        "\n",
        "    #Reshape the input so all sequences have same shape\n",
        "    x_train_source, y_train_source = unique_shapes(x_train_source, y_train_source, lag, n_features, num_of_outputs)\n",
        "    x_test_source, y_test_source = unique_shapes(x_test_source, y_test_source, lag, n_features, num_of_outputs)\n",
        "\n",
        "    x_train_target, y_train_target = unique_shapes(x_train_target, y_train_target, lag, n_features, num_of_outputs)\n",
        "    x_test_target, y_test_target = unique_shapes(x_test_target, y_test_target, lag, n_features, num_of_outputs)\n",
        "\n",
        "    x_train_target_2, y_train_target_2 = unique_shapes(x_train_target_2, y_train_target_2, lag, n_features, num_of_outputs)\n",
        "    x_test_target_2, y_test_target_2 = unique_shapes(x_test_target_2, y_test_target_2, lag, n_features, num_of_outputs)\n",
        "\n",
        "\n",
        "\n",
        "    #Save preprocessed input files\n",
        "    if i==0:\n",
        "\n",
        "        np.save('Data/Preprocessed data/12M/source_train_x.npy', x_train_source)\n",
        "        np.save('Data/Preprocessed data/12M/source_train_y.npy', y_train_source)\n",
        "        np.save('Data/Preprocessed data/12M/source_test_x.npy', x_test_source)\n",
        "        np.save('Data/Preprocessed data/12M/source_test_y.npy', y_test_source)\n",
        "\n",
        "        np.save('Data/Preprocessed data/12M/target_train_x.npy', x_train_target)\n",
        "        np.save('Data/Preprocessed data/12M/target_train_y.npy', y_train_target)\n",
        "        np.save('Data/Preprocessed data/12M/target_test_x.npy', x_test_target)\n",
        "        np.save('Data/Preprocessed data/12M/target_test_y.npy', y_test_target)\n",
        "\n",
        "        np.save('Data/Preprocessed data/12M/target_2_train_x.npy', x_train_target_2)\n",
        "        np.save('Data/Preprocessed data/12M/target_2_train_y.npy', y_train_target_2)\n",
        "        np.save('Data/Preprocessed data/12M/target_2_test_x.npy', x_test_target_2)\n",
        "        np.save('Data/Preprocessed data/12M/target_2_test_y.npy', y_test_target_2)\n",
        "\n",
        "    elif i==1:\n",
        "\n",
        "        np.save('Data/Preprocessed data/6M/source_train_x.npy', x_train_source)\n",
        "        np.save('Data/Preprocessed data/6M/source_train_y.npy', y_train_source)\n",
        "        np.save('Data/Preprocessed data/6M/source_test_x.npy', x_test_source)\n",
        "        np.save('Data/Preprocessed data/6M/source_test_y.npy', y_test_source)\n",
        "\n",
        "        np.save('Data/Preprocessed data/6M/target_train_x.npy', x_train_target)\n",
        "        np.save('Data/Preprocessed data/6M/target_train_y.npy', y_train_target)\n",
        "        np.save('Data/Preprocessed data/6M/target_test_x.npy', x_test_target)\n",
        "        np.save('Data/Preprocessed data/6M/target_test_y.npy', y_test_target)\n",
        "\n",
        "        np.save('Data/Preprocessed data/6M/target_2_train_x.npy', x_train_target_2)\n",
        "        np.save('Data/Preprocessed data/6M/target_2_train_y.npy', y_train_target_2)\n",
        "        np.save('Data/Preprocessed data/6M/target_2_test_x.npy', x_test_target_2)\n",
        "        np.save('Data/Preprocessed data/6M/target_2_test_y.npy', y_test_target_2)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
