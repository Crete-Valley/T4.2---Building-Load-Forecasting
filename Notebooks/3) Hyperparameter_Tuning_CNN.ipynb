{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-dX2KrQRC7F",
        "outputId": "d58b2fd8-db55-4ffc-9a59-3c5ee46a0146"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "\n",
        "def compute_r2_score(model, x_test, y_test):\n",
        "\n",
        "    #Utility function to compute R2 score\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    return r2_score(y_test, y_pred)*100\n",
        "\n",
        "\n",
        "\n",
        "def create_model(trial):\n",
        "\n",
        "    conv_filters_1 = trial.suggest_categorical('conv_filters_1', [32, 64, 128, 256])\n",
        "    conv_filters_2 = trial.suggest_categorical('conv_filters_2', [32, 64, 128, 256])\n",
        "    conv_filters_3 = trial.suggest_categorical('conv_filters_3', [32, 64, 128, 256])\n",
        "    dense_units_1 = trial.suggest_categorical('dense_units_1', [64, 128, 256])\n",
        "    dense_units_2 = trial.suggest_categorical('dense_units_2', [64, 128, 256])\n",
        "    dropout_rate_1 = trial.suggest_float('dropout_rate_1', 0.2, 0.5, step=0.05)\n",
        "    dropout_rate_2 = trial.suggest_float('dropout_rate_2', 0.2, 0.5, step=0.05)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(x_train_source.shape[1], x_train_source.shape[2])))\n",
        "    model.add(tf.keras.layers.Conv1D(conv_filters_1, 5, strides=1, padding='valid', activation='relu')) # 1D convolutional layer with 32 filters, a kernel size of 3, and a ReLU activation function\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Conv1D(conv_filters_2, 3, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Conv1D(conv_filters_3, 3, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(dense_units_1, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(dropout_rate_1))\n",
        "    model.add(tf.keras.layers.Dense(dense_units_2, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(dropout_rate_2))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def objective_source(trial):\n",
        "\n",
        "    model = create_model(trial)\n",
        "\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10, restore_best_weights=True)\n",
        "\n",
        "    # Suggest batch size\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
        "\n",
        "    history = model.fit(x_train_source, y_train_source, epochs=30, validation_split=0.2,\n",
        "                        batch_size=batch_size, verbose=0, shuffle=True, callbacks=[es])\n",
        "\n",
        "    # Return the r2 score of the prediction to the test data\n",
        "\n",
        "    val_r2 = compute_r2_score(model, x_test_source, y_test_source)\n",
        "\n",
        "    return val_r2\n",
        "\n",
        "\n",
        "def objective_target(trial):\n",
        "\n",
        "    model = create_model(trial)\n",
        "\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10, restore_best_weights=True)\n",
        "\n",
        "    # Suggest batch size\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
        "\n",
        "    history = model.fit(x_train_target, y_train_target, epochs=30, validation_split=0.2,\n",
        "                        batch_size=batch_size, verbose=0, shuffle=True, callbacks=[es])\n",
        "\n",
        "    # Return the r2 score of the prediction to the test data\n",
        "\n",
        "    val_r2 = compute_r2_score(model, x_test_target, y_test_target)\n",
        "\n",
        "    return val_r2\n",
        "\n",
        "\n",
        "def objective_target_2(trial):\n",
        "\n",
        "    model = create_model(trial)\n",
        "\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10, restore_best_weights=True)\n",
        "\n",
        "    # Suggest batch size\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
        "\n",
        "    history = model.fit(x_train_target_2, y_train_target_2, epochs=30, validation_split=0.2,\n",
        "                        batch_size=batch_size, verbose=0, shuffle=True, callbacks=[es])\n",
        "\n",
        "    # Return the r2 score of the prediction to the test data\n",
        "\n",
        "    val_r2 = compute_r2_score(model, x_test_target_2, y_test_target_2)\n",
        "\n",
        "    return val_r2\n",
        "\n",
        "\n",
        "def best_param(data, filename):\n",
        "\n",
        "    pruner = optuna.pruners.MedianPruner(\n",
        "    n_startup_trials=10,  # Number of trials to wait before pruning\n",
        "    n_warmup_steps=10,    # Number of epochs before pruning starts\n",
        "    interval_steps=2     # Prune every epoch after warmup\n",
        "    )\n",
        "\n",
        "    study = optuna.create_study(direction='maximize', pruner = pruner)\n",
        "    # Start the optimization\n",
        "    if data == 'source':\n",
        "        study.optimize(objective_source, n_trials=60)\n",
        "    elif data == 'target':\n",
        "        study.optimize(objective_target, n_trials=60)\n",
        "    elif data == 'target_2':\n",
        "        study.optimize(objective_target_2, n_trials=60)\n",
        "\n",
        "    # Print the best parameters\n",
        "    print('Best trial:')\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print('  Value: {}'.format(trial.value))\n",
        "    print('  Params: ')\n",
        "    for key, value in trial.params.items():\n",
        "        print('    {}: {}'.format(key, value))\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(trial.params, f)\n",
        "\n",
        "\n",
        "#load preprocessed data for model fitting\n",
        "\n",
        "x_train_source = np.load('Data/Preprocessed data/source_train_x.npy')\n",
        "y_train_source = np.load('Data/Preprocessed data/source_train_y.npy')\n",
        "x_test_source = np.load('Data/Preprocessed data/source_test_x.npy')\n",
        "y_test_source = np.load('Data/Preprocessed data/source_test_y.npy')\n",
        "\n",
        "\n",
        "best_param('source', 'Models/CNN/12M/Tuned Hyperparameters/source_params.json')\n",
        "\n",
        "\n",
        "#HYPERPARAMETER TUNING FOR 12 MONTH DATASETS\n",
        "\n",
        "x_train_target = np.load('Data/Preprocessed data/12M/target_train_x.npy')\n",
        "y_train_target = np.load('Data/Preprocessed data/12M/target_train_y.npy')\n",
        "x_test_target = np.load('Data/Preprocessed data/12M/target_test_x.npy')\n",
        "y_test_target = np.load('Data/Preprocessed data/12M/target_test_y.npy')\n",
        "x_train_target_2 = np.load('Data/Preprocessed data/12M/target_2_train_x.npy')\n",
        "y_train_target_2 = np.load('Data/Preprocessed data/12M/target_2_train_y.npy')\n",
        "x_test_target_2 = np.load('Data/Preprocessed data/12M/target_2_test_x.npy')\n",
        "y_test_target_2 = np.load('Data/Preprocessed data/12M/target_2_test_y.npy')\n",
        "\n",
        "\n",
        "#save best hyperparameters for each base model (source, target, target_2)\n",
        "\n",
        "best_param('target', 'Models/CNN/12M/Tuned Hyperparameters/target_params.json')\n",
        "\n",
        "best_param('target_2', 'Models/CNN/12M/Tuned Hyperparameters/target_2_params.json')\n",
        "\n",
        "\n",
        "#HYPERPARAMETER TUNING FOR 6 MONTH DATASETS\n",
        "\n",
        "x_train_target = np.load('Data/Preprocessed data/6M/target_train_x.npy')\n",
        "y_train_target = np.load('Data/Preprocessed data/6M/target_train_y.npy')\n",
        "x_test_target = np.load('Data/Preprocessed data/6M/target_test_x.npy')\n",
        "y_test_target = np.load('Data/Preprocessed data/6M/target_test_y.npy')\n",
        "x_train_target_2 = np.load('Data/Preprocessed data/6M/target_2_train_x.npy')\n",
        "y_train_target_2 = np.load('Data/Preprocessed data/6M/target_2_train_y.npy')\n",
        "x_test_target_2 = np.load('Data/Preprocessed data/6M/target_2_test_x.npy')\n",
        "y_test_target_2 = np.load('Data/Preprocessed data/6M/target_2_test_y.npy')\n",
        "\n",
        "\n",
        "#save best hyperparameters for each base model (source, target, target_2)\n",
        "\n",
        "\n",
        "best_param('target', 'Models/CNN/6M/Tuned Hyperparameters/target_params.json')\n",
        "\n",
        "best_param('target_2', 'Models/CNN/6M/Tuned Hyperparameters/target_2_params.json')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
